% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.9 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nyt/global//global/global}
    \entry{introduction}{misc}{}
      \field{sortinit}{A}
      \field{sortinithash}{3248043b5fe8d0a34dab5ab6b8d4309b}
      \field{labeldatesource}{nodate}
      \field{labeltitlesource}{title}
      \field{howpublished}{\url{https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/}}
      \field{note}{Published: 08-09-2016}
      \field{title}{A Quick Introduction to Neural Networks}
    \endentry
    \entry{srccode}{misc}{}
      \field{sortinit}{C}
      \field{sortinithash}{095692fd22cc3c74d7fe223d02314dbd}
      \field{labeldatesource}{nodate}
      \field{labeltitlesource}{title}
      \field{howpublished}{\url{https://github.com/Jose-R-Rodriguez/NeuralNetworkImplementation/}}
      \field{note}{Published: 06-28-2018}
      \field{title}{Codigo Fuente}
    \endentry
    \entry{feedforward}{misc}{}
      \field{sortinit}{D}
      \field{sortinithash}{d10b5413de1f3d197b20897dd0d565bb}
      \field{labeldatesource}{nodate}
      \field{labeltitlesource}{title}
      \field{howpublished}{\url{https://towardsdatascience.com/deep-learning-feedforward-neural-network-26a6705dbdc7/}}
      \field{note}{Published: 01-05-2017}
      \field{title}{Deep Learning: Feedforward Neural Network}
    \endentry
    \entry{Sigmoid}{misc}{}
      \field{sortinit}{U}
      \field{sortinithash}{1554584e7f69d2b4fe0a645e73d50194}
      \field{labeldatesource}{nodate}
      \field{labeltitlesource}{title}
      \field{howpublished}{\url{https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0}}
      \field{note}{Published: 03-30-2017}
      \field{title}{Understanding Activation Functions in Neural Networks}
    \endentry
    \entry{gradient_descent}{misc}{}
      \field{sortinit}{U}
      \field{sortinithash}{1554584e7f69d2b4fe0a645e73d50194}
      \field{labeldatesource}{nodate}
      \field{labeltitlesource}{title}
      \field{howpublished}{\url{https://eli.thegreenplace.net/2016/understanding-gradient-descent/}}
      \field{note}{Published: 08-05-2016}
      \field{title}{Understanding Gradient Descent}
    \endentry
    \entry{rndWeight}{misc}{}
      \field{sortinit}{W}
      \field{sortinithash}{6d25b3eefe5aa2147d1f339686808918}
      \field{labeldatesource}{nodate}
      \field{labeltitlesource}{title}
      \field{howpublished}{\url{https://stackoverflow.com/questions/20027598/why-should-weights-of-neural-networks-be-initialized-to-random-numbers}}
      \field{note}{Published: 12-30-2017}
      \field{title}{Why should weights of Neural Networks be initialized to random numbers?}
    \endentry
  \enddatalist
\endrefsection
\endinput

